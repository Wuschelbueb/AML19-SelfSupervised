{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomRotation, ToPILImage\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = Compose([ToTensor(), Normalize(mean=(0.5,), std=(0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'fashion_mnist'\n",
    "batch_size=64\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = FashionMNIST(root=root_dir, download=True, train=True, transform=transform)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = FashionMNIST(root=root_dir, download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTAugmentedDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, target, transform):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        angle = random.choice([0, 90, 180, 270])\n",
    "        \n",
    "        datapoint = self.transform(self.data[index], angle)\n",
    "        \n",
    "        # need to use ints for classes [0, 90, 180, 270]\n",
    "        if angle == 0:\n",
    "            rotation_class = 0 \n",
    "        if angle == 90:\n",
    "            rotation_class = 1\n",
    "        if angle == 180:\n",
    "            rotation_class = 2\n",
    "        if angle == 270:\n",
    "            rotation_class = 3\n",
    "        \n",
    "        target = rotation_class\n",
    "\n",
    "        return datapoint, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate given images by given angle\n",
    "\n",
    "def my_segmentation_transforms(image, angle):\n",
    "    image = TF.to_pil_image(image)\n",
    "    image = TF.resize(image, 32, interpolation=2)\n",
    "    image = TF.rotate(image, angle)\n",
    "    image = TF.to_tensor(image)\n",
    "    image = TF.normalize(image, (0.5, ), (0.5, ))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "n_train = int(len(trainset) * 0.8)\n",
    "\n",
    "train_augmented = FashionMNISTAugmentedDataset(\n",
    "    # next line is for real training\n",
    "    data=trainset.data[:n_train],\n",
    "    # use next line only because of performance issues\n",
    "#     data=trainset.data[:300], \n",
    "    # next line is for real training\n",
    "    target=trainset.targets[:n_train],\n",
    "    # use next line only because of performance issues\n",
    "#     target=trainset.targets[:300], \n",
    "    transform=my_segmentation_transforms)\n",
    "trainloader_augmented = DataLoader(train_augmented, batch_size=batch_size, shuffle=True, num_workers=32)\n",
    "\n",
    "val_augmented = FashionMNISTAugmentedDataset(\n",
    "    # next line is for real training\n",
    "    data=trainset.data[n_train:], \n",
    "    # use next line only because of performance issues\n",
    "#     data=trainset.data[300:400],\n",
    "    # next line is for real training\n",
    "    target=trainset.targets[n_train:],\n",
    "    # use next line only because of performance issues\n",
    "#     target=trainset.targets[300:400], \n",
    "    transform=my_segmentation_transforms)\n",
    "valloader_augmented = DataLoader(val_augmented, batch_size=batch_size, shuffle=True, num_workers=32)\n",
    "\n",
    "test_augmented = FashionMNISTAugmentedDataset(\n",
    "    data=testset.data, \n",
    "    target=testset.targets, \n",
    "    transform=my_segmentation_transforms)\n",
    "testloader_augmented = DataLoader(test_augmented, batch_size=batch_size, shuffle=True, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def train(model, loss_fn, optimizer, scheduler, num_epochs, trainloader, valloader):\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    dataloader = None\n",
    "    dataset_size = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        since = time.time()\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "                dataloader = trainloader\n",
    "                dataset_size = len(trainloader.dataset)\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                dataloader = valloader\n",
    "                dataset_size = len(valloader.dataset)\n",
    "             \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            \n",
    "            for data in dataloader:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "#                 print(\"outputs:\", outputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                \n",
    "#                 print(\"labels:\", labels)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data).to(torch.float32)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_size\n",
    "            epoch_acc = running_corrects / dataset_size\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def flatten(x): \n",
    "    return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    expansion = 1       \n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residue = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residue = self.downsample(x)\n",
    "\n",
    "        out += residue\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        residue = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(x)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residue = self.downsample(x)\n",
    "\n",
    "        out += residue\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "            \n",
    "            \n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, depth, name, num_classes=10, block=BasicBlock):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        assert (depth - 2) % 6 == 0, 'Depth should be 6n + 2'\n",
    "        n = (depth - 2) // 6\n",
    "\n",
    "        self.name = name\n",
    "        block = BasicBlock\n",
    "        self.inplanes = 16\n",
    "        fmaps = [16, 32, 64] # CIFAR10\n",
    "\n",
    "        self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, fmaps[0], n, stride=1)\n",
    "        self.layer2 = self._make_layer(block, fmaps[1], n, stride=2)\n",
    "        self.layer3 = self._make_layer(block, fmaps[2], n, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=8, stride=1)\n",
    "        self.flatten = flatten\n",
    "        self.fc = nn.Linear(fmaps[2] * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        ''' Between layers convolve input to match dimensions -> stride = 2 '''\n",
    "\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                    nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                              kernel_size=1, stride=stride, bias=False),\n",
    "                    nn.BatchNorm2d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, print_sizes=False):\n",
    "        \n",
    "        if print_sizes:\n",
    "            print('Sizes of the tensors inside each node: \\n')\n",
    "            print(\"\\t In Model: input size\", x.size())\n",
    "        \n",
    "        x = self.relu(self.bn(self.conv(x)))    # 32x32\n",
    "        \n",
    "        x = self.layer1(x)                      # 32x32\n",
    "        x = self.layer2(x)                      # 16x16\n",
    "        x = self.layer3(x)                      # 8x8\n",
    "\n",
    "        x = self.avgpool(x)                     # 1x1\n",
    "        x = self.flatten(x)                     # Flatten\n",
    "        x  = self.fc(x)                         # Dense\n",
    "        \n",
    "        if print_sizes:\n",
    "            print(\"\\t In Model: output size\", x.size())\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet20(**kwargs):    \n",
    "    return ResNet(name = 'ResNet20', depth = 20, num_classes=4,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet20 = ResNet20()\n",
    "# fitting the convolution to 1 input channel (instead of 3)\n",
    "resnet20.conv = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "train Loss: 0.0238 Acc: 0.2700\n",
      "val Loss: 0.0302 Acc: 0.2200\n",
      "Epoch 2/5\n",
      "train Loss: 0.0200 Acc: 0.4333\n",
      "val Loss: 0.0265 Acc: 0.3200\n",
      "Epoch 3/5\n",
      "train Loss: 0.0176 Acc: 0.5667\n",
      "val Loss: 0.0278 Acc: 0.3300\n",
      "Epoch 4/5\n",
      "train Loss: 0.0150 Acc: 0.6900\n",
      "val Loss: 0.0232 Acc: 0.4700\n",
      "Epoch 5/5\n",
      "train Loss: 0.0129 Acc: 0.7233\n",
      "val Loss: 0.0200 Acc: 0.6100\n",
      "Training complete in 1m 19s\n",
      "Best val Acc: 0.610000\n"
     ]
    }
   ],
   "source": [
    "# Criteria NLLLoss which is recommended with Softmax final layer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optim = torch.optim.Adam(resnet20.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 4 epochs\n",
    "sched = torch.optim.lr_scheduler.StepLR(optimizer=optim, step_size=4, gamma=0.1)\n",
    "\n",
    "# Number of epochs\n",
    "eps=5\n",
    "\n",
    "resnet20_trained = train(resnet20, loss_fn, optim, sched, eps, trainloader_augmented, valloader_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform given images for fashion classification (without rotation)\n",
    "\n",
    "def my_classification_transforms(image):\n",
    "    image = TF.to_pil_image(image)\n",
    "    image = TF.resize(image, 32, interpolation=2)\n",
    "    image = TF.to_tensor(image)\n",
    "    image = TF.normalize(image, (0.5, ), (0.5, ))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataset for fashion classification (with fashion labels)\n",
    "\n",
    "class FashionMNISTClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, target, transform):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        datapoint = self.transform(self.data[index])\n",
    "        targetpoint = self.target[index]\n",
    "\n",
    "        return datapoint, targetpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader for classification -> take only a subset\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "# generate random indices for a 10% subset\n",
    "number_trainInst = int(len(trainset))\n",
    "subset_indices = np.random.choice(number_trainInst, number_trainInst/10, replace=False)\n",
    "subset_data = torch.zeros([number_trainInst/10, 28, 28], dtype=torch.float)\n",
    "subset_targets = np.zeros(number_trainInst/10, dtype=long)\n",
    "# create subset for 10% of the original training set\n",
    "for i, index in enumerate(subset_indices):\n",
    "    subset_data[i] = trainset.data[index]\n",
    "    subset_targets[i] = trainset.targets[index]\n",
    "\n",
    "n_train = int(len(subset_data) * 0.8)\n",
    "\n",
    "train_augmented_classification = FashionMNISTClassificationDataset(\n",
    "    data=subset_data[:n_train], \n",
    "    # use next line only because of performance issues\n",
    "#     data=subset_data[:50], \n",
    "    target=subset_targets[:n_train], \n",
    "    # use next line only because of performance issues\n",
    "#     target=subset_targets[:50], \n",
    "    transform=my_classification_transforms)\n",
    "\n",
    "trainloader_classification = DataLoader(train_augmented_classification, batch_size=batch_size, shuffle=True, num_workers=32)\n",
    "\n",
    "val_augmented_classification = FashionMNISTClassificationDataset(\n",
    "    data=subset_data[n_train:], \n",
    "    # use next line only because of performance issues\n",
    "#     data=subset_data[100:150],\n",
    "    target=subset_targets[n_train:], \n",
    "    # use next line only because of performance issues\n",
    "#     target=subset_targets[100:150], \n",
    "    transform=my_classification_transforms)\n",
    "\n",
    "valloader_classification = DataLoader(val_augmented_classification, batch_size=batch_size, shuffle=True, num_workers=32)\n",
    "\n",
    "test_augmented_classification = FashionMNISTClassificationDataset(\n",
    "    data=testset.data, \n",
    "    target=testset.targets, \n",
    "    transform=my_classification_transforms)\n",
    "\n",
    "testloader_classification = DataLoader(test_augmented_classification, batch_size=batch_size, shuffle=True, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "train Loss: 2.4916 Acc: 0.0800\n",
      "val Loss: 2.6453 Acc: 0.1200\n",
      "Epoch 2/5\n",
      "train Loss: 2.2884 Acc: 0.1800\n",
      "val Loss: 2.6369 Acc: 0.1200\n",
      "Epoch 3/5\n",
      "train Loss: 2.2528 Acc: 0.1600\n",
      "val Loss: 2.5940 Acc: 0.1800\n",
      "Epoch 4/5\n",
      "train Loss: 2.2585 Acc: 0.1400\n",
      "val Loss: 2.6459 Acc: 0.1400\n",
      "Epoch 5/5\n",
      "train Loss: 2.1968 Acc: 0.1800\n",
      "val Loss: 2.6149 Acc: 0.1400\n",
      "Training complete in 0m 11s\n",
      "Best val Acc: 0.180000\n"
     ]
    }
   ],
   "source": [
    "# Criteria NLLLoss which is recommended with Softmax final layer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# freeze all layers of the trained model\n",
    "for param in resnet20_trained.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# # unfreeze layer3\n",
    "# for param in resnet20_trained.layer3.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# unfreeze final fc layer\n",
    "for param in resnet20_trained.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# replace fc layer with 10 outputs\n",
    "resnet20_trained.fc = nn.Linear(64, 10)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optim = torch.optim.Adam(resnet20_trained.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 4 epochs\n",
    "sched = torch.optim.lr_scheduler.StepLR(optimizer=optim, step_size=4, gamma=0.1)\n",
    "\n",
    "# Number of epochs\n",
    "eps=5\n",
    "\n",
    "resnet20_trainedClassification = train(resnet20_trained, loss_fn, optim, sched, eps, trainloader_classification, valloader_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize the results graphically\n",
    "\n",
    "def stats_plot(trainLoss, valLoss, valAccuracy):\n",
    "    # x axis is number of epochs\n",
    "    x = np.arange(0, len(valLoss), 1)\n",
    "\n",
    "    fig=plt.figure(figsize=(12, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    plt.figure(1)\n",
    "\n",
    "    plt.subplot(221)\n",
    "    plt.ylabel('training loss', fontsize=14, color='black')\n",
    "    plt.grid(True)\n",
    "    plt.plot(x, trainLoss, 'b', linewidth=2)\n",
    "\n",
    "\n",
    "    plt.subplot(223)\n",
    "    plt.ylabel('validation loss', fontsize=14, color='black')\n",
    "    plt.plot(x, valLoss, 'r', linewidth=2)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoches', fontsize=14, color='black')\n",
    "\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.ylabel('validation accuracy', fontsize=14, color='black')\n",
    "    plt.plot(x, valAccuracy, 'g', linewidth=3)\n",
    "    #plt.hlines(y=46.5, color='orange', label = '46.4%% threshold')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoches', fontsize=14, color='black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
