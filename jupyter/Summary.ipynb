{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torchvision.transforms.functional as tf\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, Subset, random_split\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import FashionMNIST, ImageFolder\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomRotation, ToPILImage, RandomHorizontalFlip, RandomCrop, ColorJitter, RandomResizedCrop, RandomRotation, \\\n",
    "    RandomAffine, Resize\n",
    "from matplotlib import pyplot as plt\n",
    "from random import randint\n",
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([Resize(32), ToTensor(), Normalize(mean=(0.5,), std=(0.5,))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index][0]\n",
    "        target = self.targets[index][1]\n",
    "\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = 'fashion_mnist'\n",
    "TARGET_SIZE = 32\n",
    "TRANSFORM = Compose([Resize(TARGET_SIZE), ToTensor(), Normalize(mean=(0.5,), std=(0.5,))])\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Download and load the training data\n",
    "train_data = FashionMNIST(root=ROOT_DIR, download=True, train=True, transform=transform)\n",
    "test_data = FashionMNIST(root=ROOT_DIR, download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_subset(split):\n",
    "    \"\"\"Splits the train data in train and validation subsets.\"\"\"\n",
    "    data_set = train_data\n",
    "    nbr_train_examples = int(len(data_set) * 0.05)\n",
    "    nbr_val_examples = int(len(data_set) * 0.05)\n",
    "    nbr_unused_examples = int(len(data_set) * 0.9)\n",
    "    \n",
    "    return random_split(data_set, [nbr_train_examples, nbr_val_examples, nbr_unused_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loader_fashion_mnist():\n",
    "    \"\"\"Creates the data loader for the train data.\"\"\"\n",
    "    train_subset, _, _ = train_val_subset(0.8)\n",
    "\n",
    "    train_set = FashionMNISTDataset(\n",
    "        data=train_subset,\n",
    "        targets=train_subset\n",
    "    )\n",
    "    print(len(train_set))\n",
    "\n",
    "    return DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loader_fashion_mnist():\n",
    "    \"\"\"Creates the data loader for the validation data.\"\"\"\n",
    "    _, val_subset, _ = train_val_subset(0.8)\n",
    "\n",
    "    val_set = FashionMNISTDataset(\n",
    "        data=val_subset,\n",
    "        targets=val_subset\n",
    "    )\n",
    "    print(len(val_set))\n",
    "\n",
    "    return DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loader_fashion_mnist():\n",
    "    \"\"\"Creates the data loader for the test data.\"\"\"\n",
    "    return DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "3000\n",
      "torch.Size([256, 1, 32, 32]) torch.Size([256])\n",
      "tensor([9, 8, 3, 5, 1, 6, 6, 1, 4, 5, 2, 9, 5, 6, 7, 5, 8, 4, 7, 8, 2, 7, 9, 1,\n",
      "        8, 5, 9, 2, 8, 3, 7, 9, 0, 5, 0, 5, 5, 5, 1, 8, 8, 0, 0, 2, 8, 9, 3, 9,\n",
      "        8, 7, 0, 3, 2, 0, 8, 1, 9, 4, 0, 7, 3, 8, 5, 1, 6, 7, 0, 1, 9, 9, 9, 4,\n",
      "        5, 9, 9, 4, 3, 1, 6, 9, 4, 7, 6, 8, 5, 0, 5, 0, 9, 5, 7, 3, 5, 8, 3, 2,\n",
      "        1, 8, 4, 2, 4, 1, 9, 9, 1, 3, 8, 1, 0, 4, 1, 3, 4, 3, 7, 1, 5, 1, 5, 5,\n",
      "        4, 9, 8, 2, 9, 5, 3, 3, 2, 2, 8, 7, 4, 4, 2, 6, 9, 0, 6, 6, 5, 9, 5, 1,\n",
      "        9, 4, 4, 1, 6, 8, 3, 2, 7, 4, 4, 0, 0, 7, 7, 2, 4, 0, 0, 4, 1, 3, 6, 0,\n",
      "        0, 9, 8, 6, 3, 3, 7, 8, 1, 9, 8, 8, 1, 9, 2, 2, 5, 7, 7, 0, 3, 7, 0, 9,\n",
      "        7, 0, 7, 2, 4, 3, 0, 1, 7, 4, 1, 8, 5, 2, 5, 2, 0, 8, 3, 1, 6, 9, 9, 5,\n",
      "        9, 4, 1, 2, 9, 8, 6, 9, 2, 1, 6, 8, 1, 3, 1, 1, 1, 5, 0, 8, 4, 3, 8, 8,\n",
      "        3, 2, 8, 7, 6, 1, 5, 6, 3, 0, 3, 7, 7, 4, 8, 0])\n",
      "torch.Size([256, 1, 32, 32]) torch.Size([256])\n",
      "tensor([1, 4, 5, 5, 7, 4, 4, 7, 0, 5, 5, 9, 1, 9, 3, 2, 4, 5, 7, 7, 3, 8, 8, 1,\n",
      "        0, 8, 1, 7, 0, 6, 5, 0, 7, 3, 4, 6, 9, 1, 0, 2, 6, 7, 6, 4, 7, 2, 6, 9,\n",
      "        5, 0, 9, 5, 4, 0, 2, 1, 2, 5, 1, 2, 5, 5, 6, 3, 8, 9, 8, 6, 2, 4, 6, 5,\n",
      "        6, 3, 2, 3, 7, 6, 0, 1, 9, 5, 7, 6, 8, 3, 6, 5, 9, 4, 5, 3, 8, 3, 0, 4,\n",
      "        0, 2, 1, 7, 7, 9, 6, 6, 8, 1, 7, 1, 7, 2, 0, 0, 0, 5, 7, 9, 8, 6, 7, 7,\n",
      "        0, 3, 2, 3, 4, 9, 2, 2, 5, 0, 2, 6, 3, 5, 3, 6, 6, 5, 9, 3, 4, 4, 3, 2,\n",
      "        3, 7, 0, 0, 2, 1, 2, 1, 8, 2, 7, 3, 1, 1, 6, 6, 3, 9, 0, 8, 4, 6, 0, 7,\n",
      "        5, 0, 5, 7, 0, 9, 3, 1, 6, 6, 2, 6, 9, 1, 4, 1, 1, 0, 1, 4, 4, 0, 6, 7,\n",
      "        0, 5, 3, 6, 6, 5, 5, 4, 7, 0, 8, 6, 1, 8, 1, 6, 2, 4, 6, 4, 8, 8, 2, 2,\n",
      "        1, 3, 3, 5, 6, 7, 3, 6, 5, 1, 6, 8, 4, 5, 4, 8, 5, 2, 4, 6, 7, 8, 9, 0,\n",
      "        1, 0, 0, 3, 1, 2, 3, 7, 5, 2, 2, 5, 7, 0, 7, 3])\n",
      "torch.Size([256, 1, 32, 32]) torch.Size([256])\n",
      "tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 7, 3, 4, 1, 2, 4, 8, 0, 2, 5, 7, 9,\n",
      "        1, 4, 6, 0, 9, 3, 8, 8, 3, 3, 8, 0, 7, 5, 7, 9, 6, 1, 3, 7, 6, 7, 2, 1,\n",
      "        2, 2, 4, 4, 5, 8, 2, 2, 8, 4, 8, 0, 7, 7, 8, 5, 1, 1, 2, 3, 9, 8, 7, 0,\n",
      "        2, 6, 2, 3, 1, 2, 8, 4, 1, 8, 5, 9, 5, 0, 3, 2, 0, 6, 5, 3, 6, 7, 1, 8,\n",
      "        0, 1, 4, 2, 3, 6, 7, 2, 7, 8, 5, 9, 9, 4, 2, 5, 7, 0, 5, 2, 8, 6, 7, 8,\n",
      "        0, 0, 9, 9, 3, 0, 8, 4, 1, 5, 4, 1, 9, 1, 8, 6, 2, 1, 2, 5, 1, 0, 0, 0,\n",
      "        1, 6, 1, 6, 2, 2, 4, 4, 1, 4, 5, 0, 4, 7, 9, 3, 7, 2, 3, 9, 0, 9, 4, 7,\n",
      "        4, 2, 0, 5, 2, 1, 2, 1, 3, 0, 9, 1, 0, 9, 3, 6, 7, 9, 9, 4, 4, 7, 1, 2,\n",
      "        1, 6, 3, 2, 8, 3, 6, 1, 1, 0, 2, 9, 2, 4, 0, 7, 9, 8, 4, 1, 8, 4, 1, 3,\n",
      "        1, 6, 7, 2, 8, 5, 2, 0, 7, 7, 6, 2, 7, 0, 7, 8, 9, 2, 9, 0, 5, 1, 4, 4,\n",
      "        5, 6, 9, 2, 6, 8, 6, 4, 2, 2, 9, 7, 6, 5, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "train_loader = train_loader_fashion_mnist()\n",
    "val_loader = val_loader_fashion_mnist()\n",
    "test_loader = test_loader_fashion_mnist()\n",
    "\n",
    "for xs, ys in train_loader:\n",
    "    print(xs.size(), ys.size())\n",
    "    print(ys)\n",
    "    break    \n",
    "    \n",
    "for xs, ys in val_loader:\n",
    "    print(xs.size(), ys.size())\n",
    "    print(ys)\n",
    "    break\n",
    "\n",
    "for xs, ys in test_loader:\n",
    "    print(xs.size(), ys.size())\n",
    "    print(ys)\n",
    "    break        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(image, angle):\n",
    "    \"\"\"Rotate the image by the specified angle\"\"\"\n",
    "    image = tf.to_pil_image(image)\n",
    "    image = tf.rotate(image, angle)\n",
    "    image = tf.to_tensor(image)\n",
    "    image = tf.normalize(image, (0.5,), (0.5,))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 32, 32])\n",
      "torch.Size([512])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuwAAAIeCAYAAADgelruAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3XmY3WV9///XnWVmksxk30MWAgSSQBA1SAqoLBXBBbAFi9i6Vq0/LkXELi4V64Kl1X6t1VIVvj8ELHVFUBFEBERRCZIEQlhDQvZkJplM1skkc3//OCd1jPfrTuY4mbkTno/ryqV53/M+5zOfc+7Pec+ZnBchxigAAAAAZerX1wcAAAAAwGNgBwAAAArGwA4AAAAUjIEdAAAAKBgDOwAAAFAwBnYAAACgYAzsfSiEcFUI4aa+Po4ShBBmhRDmhxDCAXztnBDCL3vjuICewn7/nW7u99eFEP6nN44L6Cns999hv/cMBvZeEEJ4U/XJujWEsCaEcEcI4bQ+OI5lIYSze/t+D9AnJf1rjDGGEOpDCNeFEJaHELaEEBaEEM7d+4UxxkWSWkMIr+u7wwXS2O8H5H/3uySFEEaGEL4XQthW3fdv2vuFMcbbJc0OIczpq4MFHPb7AWG/9wAG9oMshHCFpP8j6TOSxkmaIunLks7vy+MqRQhhQAhhgqQzJN1aLQ+QtELSKyQNk/RRSd8MIUzr0nqzpHf33pEC+8d+zzP7XZK+JGmXKufsUkn/GUKY3WX9vyW9q9cOFDgA7Pc89nsPizHy5yD9UWXY3CrpIrN+laSbuvz9W5LWStos6X5Js7usnSfpcUlbJK2SdGW1PlrSDyS1Stoo6eeS+iXu60ZJnZJ2VI/pb6v1UyT9stq/UNIru/Tcq8pPxr+o3u9dkkZX1xok3SSppdr7kKRx1bWJkm6rHs8zkv56n+/529XeNknvlPRXku7ez7lcJOnPuvx9UvV7qe/rx5k//ImR/V7rfpc0RJUX7xn7HP9nu/z9VEnP9fVjzB/+7P3Dfme/9/Yf3mE/uOap8sT/3gF+/R2SjpE0VtJvVXkXea/rJL07xtgk6XhJ91TrH5S0UtIYVX5a/bCkvb92+nII4cuSFGP8S0nPS3pdjLExxnhNCGGSpB9K+pSkkZKulPSdEMKYLvf7Jklvqx5TXfVrJOktqlywJksaJek9qlwsJOmW6jFNlPTnkj4TQjizy22er8qmHl79Hk+Q9KQ7KSGEcZJmSFq8txZjXCWpQ9Kxrg/oZez32vb7DEm7Y4xPdaktlNT1HbclkqaFEIb+wVkE+gb7nf3eqxjYD65RkppjjLsP5ItjjNfHGLfEGNtV+Un1xBDCsOpyh6RZIYShMcZNMcbfdqlPkDQ1xtgRY/x5rP6IGmN8b4zxvZm7fLOkH8UYfxRj7Iwx/kTSfFV+2t/r/8YYn4ox7pD0TUkv6nK/oyQdHWPcE2N8OMbYFkKYrMpPx38XY9wZY1wg6Wuq/JS914Mxxlur97lDlY29JXWAIYSBqmz6G2KMT+yzvKXaC5SA/V7bfm9U5d24rjZLaury971fz35HKdjv7PdexcB+cLVIGh1CGLC/Lwwh9A8hfDaE8GwIoU3SsurS6Or//pkqG215COG+EMK8av1fVPm11F0hhKUhhL/vxvFNlXRRCKF17x9Jp6lygdhrbZf/v12VzSZVfoV1p6RbQgirQwjXVIfriZI2xhi7btDlqvwTlr1W7HMcm/T7m1WSFELoV72fXZIuSxx/kyq/rgNKwH6v6O5+3ypp33fShur3X+T3fj37HaVgv1ew33sJA/vB9aCkdkkXHMDXvkmVXyWdrcqvoqZV60GSYowPxRjPV+VXV7eq8tOwqj+xfzDGOF3S6yVdEUI4y9xH3OfvKyTdGGMc3uXPkBjjZ/d3sNWf9j8RY5wl6U8kvVaVn7JXSxoZQui6Qaeo8u/y3HEsUuXXZP8rhBBU+TXhOFX+7XrHPuuTVPkVnv2nNEAvY79XdHe/PyVpQAjhmC61E9Xln8BJmilpWYxx33fmgL7Cfq9gv/cSBvaDKMa4WdI/SvpSCOGCEMLgEMLAEMK5IYRr9vnyJlU2f4ukwap86lySFEKoCyFcGkIYVh1c21T5gIlCCK8NIRxdHXA3S9qzdy1hnaTpXf5+k6TXhRDOqb4D0BBCeGUI4Yj9fW8hhDNCCCeEEPpXj6dDUmeMcYUqH3K5unp7cyS9o3pfzk8kvTiE0NCl9p+qbNrXVX+ttq9XSLqn+utFoM+x32vb7zHGbZK+K+mfQghDQginqjLc3Nil5xWq/BtgoAjsd/Z7b2NgP8hijJ+TdIUq0YQbVPmp9zL9fsSRJH1dlV8trVLl0+K/2mf9LyUtq/467T2qRCFJlQ+x3K3Kr5kelPTlGOPPJCmEcG0I4dout3G1pI9Wfz12ZXXzna/KB1n2HtuHdGDPi/GqfLCkTZUPiNyn3224S1R5B2G1Kh/I+XiM8W53QzHGdap8yOb86nFPVSWy8UWS1oZKvu3WEMKlXdoulXTtH9wY0IfY793f71XvlTRI0npVIt3+JsbY9R23SyT91wEcJ9Br2O/s994Uqp9fAPpUCGGWpBsknRz386Ss/lT/XzHGebmvA1Cmbu7310n6yxjjxb1ycAB6FPu9ZzCwAwAAAAXjn8QAAAAABWNgBwAAAArGwA4AAAAUjIEdAAAAKNh+/wtdPalfv358wlVSY2OjXfv85z+frO/cudP2vO9970vW6+rqbM+YMWOS9TVr1tier3zlK3btE5/4RLL+sY99zPa8613vsmvI6+zsDH19DPsTQjjs9nu/fun3ODo7XTRybcaOHWvX5syZk6wfffTRtmf58uXJ+q5du2zPzJkz7dqqVauS9Yceesj2rFy50q7VorceixLEGNnvwAuE2++8ww4AAAAUjIEdAAAAKBgDOwAAAFAwBnYAAACgYAzsAAAAQMFCjL33wW5SYir++7//267dcsstyfq8efNsz5YtW5L1E044wfa4hIXdu3d3+34kn8ywevVq2/PrX/86Wf/0pz9te+bOnWvXXkhIiTl43N6QaksgmTRpkl0799xzk/Wzzz7b9rg0mPr6etuzYMGCZD2XWDV9+nS71t7enqxv2LDB9vzoRz9K1r/xjW/Ynk2bNtk1JwS/NXrz9a4nkRIDvHCQEgMAAAAcghjYAQAAgIIxsAMAAAAFY2AHAAAACsbADgAAABRsQF8fwAvRtm3b7NqePXuS9VNOOcX2TJgwIVnPpV249IUhQ4bYnkceecSuLVmyJFk/+eSTbc+YMWOS9aamJtsDHGy1JIm8+c1vtmuvetWr7Nrs2bOT9dGjR9ueurq6ZD2X8DRjxoxkfdCgQbZnxIgRds3JJeK469RJJ51ke+677z67duONNybruccvlyDjHKrJMgAOL7zDDgAAABSMgR0AAAAoGAM7AAAAUDAGdgAAAKBgDOwAAABAwRjYAQAAgIIR69gHFixYYNeefvrpHrufdevW2bXNmzd3+/ZaWlrs2mOPPZasH3PMMbZn48aNyfrixYu7d2BAD+rfv79dO/3005P1t7zlLbbn2GOPtWv19fXJei5KcNeuXcm6i4SVpIkTJ3a7JxcT6SJjc+fOHYOLd5WkI4880q6569vPfvYz2+POa2dnZ7d7gMPJqaeeatdcHOratWttz9atW7t9DO7aJvl54YWEd9gBAACAgjGwAwAAAAVjYAcAAAAKxsAOAAAAFIyBHQAAACgYKTF9YM2aNXbNJTMMHjy42/eTSz4YMCD90OfSY1yPJHV0dHS7Z+fOncn6hg0bbA9wsA0cONCuXXDBBcn6rFmzbI9LWJB88oHbG5LU3t6erOdSGXbs2JGsDxs2zPbkDBkyJFlvaGiwPe5aMGjQINtz3HHH2bULL7wwWX/ggQdsjzt3JMHgUJO7rtTyfH75y19u19z80dzcbHvcNSeXTOX2pyQtW7YsWc+lWbm1trY225NLo3HX2NwxuMcidx4c3mEHAAAACsbADgAAABSMgR0AAAAoGAM7AAAAUDAGdgAAAKBgDOwAAABAwYh17AMtLS12zcUD1dfX255cfKPjbm/+/Pm2x0W55dZyPS4qql8/fo5E3+nfv79dO+GEE5L1XHzprl277Npzzz2XrK9fv972uONrbW21PQsXLkzWTz/9dNuTi4ZzcZCjRo2yPU5dXZ1dmzRpkl2bOXNmsp57/GqJUgP6kotvzMU6Ork5Yty4cXZtxIgRyfpRRx1le9xe2759u+3JXUfd9S13Hlzc9Nq1a23PypUr7ZqL5Hb3I/moygULFtgeh8kIAAAAKBgDOwAAAFAwBnYAAACgYAzsAAAAQMEY2AEAAICCMbADAAAABSPWsQ/kIttWrFiRrG/atMn2jBkzJlnPxaW5+KSlS5fantmzZ9s1F2uUizsivhF9ycWB5aLFXGxhrmfbtm12bdWqVcn6hg0bbM/06dOT9bFjx9qeGTNmJOu5Pf3888/btVoi5bZu3ZqsuyhbSZoyZYpdc9e9pqYm2+Mei1qicYFSuf2Zi27M7WkX8bp582bbs3HjxmTdXQckHxcrSYMGDUrWc9c9F/E6bdo02+NiGCVpx44dds1xUZDEOgIAAACHGQZ2AAAAoGAM7AAAAEDBGNgBAACAgjGwAwAAAAUjJaYP5BJfdu7cmazfdttttmfkyJHJei75wN2P+yS2lD/u+vr6ZH3IkCG2x6VD5FIjgJ7iUoqGDh1qe9z+yO21gQMH2rWXvexlyXouscElKeSO4bzzzkvW9+zZY3umTp1q19z1I5cA0djYmKznzk8uScpd9+bOnWt7fvrTnybrueMG+pJLaHF1ye8pl3IlSW1tbXbNpWC560Ctx5Db7w0NDd2qS1Jzc3Oynrvu5a69bm3Lli22Z8mSJXatu3iHHQAAACgYAzsAAABQMAZ2AAAAoGAM7AAAAEDBGNgBAACAgjGwAwAAAAUj1rEPrFu3zq49+uijyfozzzxjeyZOnJis52LeNm/enKznItFykY/Lli2za86OHTuS9VxEEtBT6urqknW3nyQfX5qLI3P3I0mTJk1K1tevX297Fi9enKz379/f9jQ1NSXr9913n+1xsYmSdMQRRyTrRx55pO2ZMGFCsp6Lp8udVxcbd8wxx9ieX/ziF8k6sY7oDbnIQMftj1wc6vjx45P1MWPG2J7BgwfbNXf9yM0Y7vU9Jxfp7G6vvb3d9rjIR3cd39/tuePbtm2b7XnyySftWnfxDjsAAABQMAZ2AAAAoGAM7AAAAEDBGNgBAACAgjGwAwAAAAUjJaYP5D5Z7dZ27dple9wnlHOfInfJFe7T4Ln7kfynp3Of+nZJNbkeoKcMGJC+/LkElFzPnj17bE8uGcIluzQ3N9uexx57LFkfO3as7Rk9enSynkuwWbt2rV3r6OhI1ocNG2Z7Ghsbk/Vcuk3u3Ln0jFxSTS4dAugJuedsLvXIcYkv5513nu0588wzk/XcNSK3D91xL1++3Pa4VLncXJK7PTcXbNq0yfYMGTIkWXfX8f1x17dcylRra2tN95XCO+wAAABAwRjYAQAAgIIxsAMAAAAFY2AHAAAACsbADgAAABSMgR0AAAAoGLGOhclFPjoucslFr0nShg0bkvVcRFIubtH15Y6hpaWlW3WgJ7lor3HjxtkeF4+4Y8cO25OLZHVxkLn4tRe/+MXJei6S1UVVnnbaabZn3bp1ds3FpY0YMcL2OLmou1zUnDNx4kS7VmucG9ATXBSpq0v+mnPKKafYHrcHcnstdwwuinH69Om2x8U6tre3257c/LFx40a75uRia3tSLvI6dx3tLt5hBwAAAArGwA4AAAAUjIEdAAAAKBgDOwAAAFAwBnYAAACgYAzsAAAAQMHIuCrMli1bkvXNmzfbHhe3mIuIdDFEa9eutT0jR460a8uWLUvWc5FG7hhy8ZFAT3ERf0OHDrU9LiZs586dtieE0L0DkzR48GC7NmHChG7fntuHw4YNsz0uli23lotRc5FyuTi5HNeXOz8ung7ojlr2tORfk3PPS7eWu+a419ZcZPLw4cPtmnt9nzx5su150YtelKznolpbW1vt2ooVK5L13GNRS+R1LobXRVLm5pxc5G938Q47AAAAUDAGdgAAAKBgDOwAAABAwRjYAQAAgIIxsAMAAAAFIyWmMNu3b0/WN23aZHtcskwuscHJfYp8ypQpdm3lypXJuvt0ueS/J/fJbqAnubSCXEpMLnmpFu65vmHDBtvz3HPPJesjRoywPfPnz0/WZ86caXty15ympqZk/aijjrI9o0aNStZzKTG5NfdY5BJ2XDIQ0FNqSZDJJTK5dLbGxkbb465t06ZNsz2523NzQX19ve1x6Ta5lBh3jZB8QoubfyR/fc0lt+zZs8euuQSZrVu32p6exGQEAAAAFIyBHQAAACgYAzsAAABQMAZ2AAAAoGAM7AAAAEDBGNgBAACAgpFxVZht27Yl6+vXr7c9mzdvTtZdHJQk7d69O1l3sZKSj2mSfMSa+34k/z3lYp+AnuKeZ7l4xFzkl5OLJnSxY83Nzbbn8ccfT9ZnzJhhe5YvX56sT5gwwfY8/PDDds2do2HDhtkeF9mWi3HNReS5x4LrBw623J7OPf9cXy4e0e3Ro48+utvHMGvWLNuTe+1va2tL1nPnYd26dcm6mz0kaePGjXatlvhqd23p6WtER0dHj96ewzvsAAAAQMEY2AEAAICCMbADAAAABWNgBwAAAArGwA4AAAAUjJSYwqxcuTJZf+6552zPzp07k/X29nbb4z5xPWbMGNuTu73p06cn6xMnTrQ97pPnufsBekotKTG1PDdziQQuBWXs2LG2x6VD5BJaXvOa1yTrkydPtj3Dhw+3ay6hJdfj1JrY4BIqGhsbe/y+gAPlEtNyGhoa7NrgwYOTdfe6L/n9+dvf/tb27Nq1y64tWLCg28fgrgWDBg2yPbnEl61btybrQ4YMsT3uGpG7DuSOz6Xetba22p6exDvsAAAAQMEY2AEAAICCMbADAAAABWNgBwAAAArGwA4AAAAUjIEdAAAAKBixjoVZvnx5sv7000/bnpaWlmQ9F3c0fvz4ZH3UqFG2x8UqSdKsWbO6fXvr169P1l3UHdCT+vVLv1/hYtQkHwfmbmt/du/enazn4hanTZuWrLsIM8nvqY6ODtszdepUu+bkYi/d95qLwRs4cKBdc99TLhrOPX65a07uvAL7qiXWMXfNca+hLrpRku67775k3b3uS9JLXvISu3bmmWcm6y6aWfJzSW6OyO1ddx5y1zD3WOQiLGu5fuQei57EO+wAAABAwRjYAQAAgIIxsAMAAAAFY2AHAAAACsbADgAAABSMlJjCPPXUU8n6qlWrbM9nP/vZZH3btm22Z8yYMcl67tPqK1assGuPPfZYst7U1GR7XPpCrYkbQE9oaGiwa/X19cn6o48+anvWrVtn14466qhkfezYsbbHpSzkkk5ceksu3WDAAP/y4O4rdwzu3OWSYHIJLbVcP9waKTHoS7nXavfav3r1atvT3NycrOf22h133GHXLr744mTdJVZJ0tq1a5P15557zvYMHz7crrmEm0GDBtmeHTt2JOsbNmyo6RhmzJiRrB977LG25/bbb7dr3cVkBAAAABSMgR0AAAAoGAM7AAAAUDAGdgAAAKBgDOwAAABAwRjYAQAAgIIR61gYFwHnotwkH7+2bNky2+NijV784hfbnlxc2qRJk5L1XIxUW1ubXQMOtv79+yfrdXV1tqezszNZz0X/uZ7c2u7du23Prl27kvXcXnOxhbVEN+5vzeno6EjWc+cud3xO7tgaGxu7fT/ufOPwUMtzOaeWGNCpU6fatVe/+tXJ+llnnWV75syZk6znji133Tv++OOT9dx1ysU6trS02J4tW7bYNTcvjBo1yva4fX3kkUfaHhe1K/nIx9zj15N4hx0AAAAoGAM7AAAAUDAGdgAAAKBgDOwAAABAwRjYAQAAgIIxsAMAAAAFI9axMM3Nzcm6i0iSpPb29mQ9F/PmIhpzUZA5I0aMSNZzkVk7duyo6b6AvuIi/saNG2d73N6QfLRkLsbVye01F+eW69mzZ49dqyVuMXd7Ti5K1sl9T4MHD07WiXX84/X086+31BLDWCv3/a5Zs8b2/OIXv0jW3bVDkp588slkfcyYMbbn6KOPtmuPPfaYXXO2bduWrLtoVSk/swwaNChZz11Xhg0blqxPnjzZ9uTOkTvnLu4xt9ba2mp7HN5hBwAAAArGwA4AAAAUjIEdAAAAKBgDOwAAAFAwBnYAAACgYKTEFKalpSVZf/rpp22P+4T5lClTbI9LX1i9enXm6LwhQ4Yk6zt37rQ9mzdvrum+gJ7g0gVcuoHkE5lcgoEkNTQ02DW3P3LJJD2ZrJFLyMitdXZ2Juu5tBXXk0t5cD2SPw+5466rq0vWa0mjwe/LnfcS0mB6knseSdLEiRPt2rHHHpus55Kk6uvrk/Va9louba6jo8OurVixIlnPnQcnd9zue5V8QkvuWunSu0aPHm17csfnnsfjx4+3Pccdd1yy/qtf/cr2OFylAAAAgIIxsAMAAAAFY2AHAAAACsbADgAAABSMgR0AAAAoGAM7AAAAUDBiHQvjYt6efPJJ27NgwYJkPRc15OKdcjFqQ4cOtWsu1jEXI7Vp0ya7BhxsLoZu+/btticXQejk9pQ7hp6Owavl9nq6x0Un5s5pbs3dV+4YXMSmi4xDeXLRf+51SJKampqS9WHDhtkeF9eai2F0UYKSNGvWrGT9iCOOsD2jRo1K1nOxzbt3707WlyxZYntycbZuH+Yia93jlOvJxUS62Mlcz7Rp07p9DLkoahf5mJu15syZk6wT6wgAAAAcZhjYAQAAgIIxsAMAAAAFY2AHAAAACsbADgAAABSMlJhDxBNPPGHXFi9enKxfeeWVtmflypXJeu7TzoMHD7ZrLgnjnnvusT2PPPKIXQP6ikszkXy6QC5hwSXBSH7f5HpcCkotiSo5ufNQyzF097b2d3su2SWX+DJmzJhk3aWBSNLmzZvtGg6MS2/JpaO4VLJcEox7fCVp0qRJ3apLPukk93zZunWrXXPPzeHDh9ueY445plvHJkkjR47ssR7Jv/YPHDjQ9rjv1SWt7O/2XPJN7rFwx51LgmlubrZr7rrsEmwkafr06Xatu3iHHQAAACgYAzsAAABQMAZ2AAAAoGAM7AAAAEDBGNgBAACAgjGwAwAAAAUj1rEP5OLSnMcee8yu/eIXv0jWczFNa9euTdZHjRple1ykneQjl3784x/bnte//vV2DTjY3HPW7Q3Jx6HmouZysYUuJiwX69jd28odg4uVrPW+ajnu3PUwd3stLS3J+pYtW2xPLfF0ODC5c/jSl740WT/vvPNsz4QJE3r0GFx04rBhw2zPxo0bk/X169fbnlys6Pbt25P1Z5991va4KMZp06bZHrdvRowYYXvGjRtn19xrf+57ddfX9vZ225OLW3QRr7lIxUcffTRZz0Vvrlq1yq7t2LEjWXcRpJKfw+rq6myPwzvsAAAAQMEY2AEAAICCMbADAAAABWNgBwAAAArGwA4AAAAUjIEdAAAAKFioJYarVv369eu9OytYLbGOOVdddVW372fAgHSiZy7iKsdFrM2ZM8f2vOENb0jWXRwUfqezs9NnBRYihHBI7vdcVNlxxx2XrH/961+3Pbl4OhdJVktEYy4+0q3l7icX+eiuLbVEWObOdy7m7eabb07WP/OZz9R0eyWLMRa/34888kj7ZLrmmmuS9dzzr7m5OVnPxQJu27bNrrlYwKamJtvjovdaW1ttT+41tJbbc8/Z8ePH2x4XR+nOgeRjLyX/OOWiCV2Eas7mzZvtmotUdHVJGjRoULePwcVoSv485GJ9XeTjRz7yEdvz9NNPJ/c777ADAAAABWNgBwAAAArGwA4AAAAUjIEdAAAAKBgDOwAAAFCwdFQIijNixAi75j4t/vTTT9set5b7xPyJJ55o19wnoefNm2d7TjjhhGT9kUcesT3AwZZLLXGpR7lkiFqSU0qQO24nl0yVS51xcikULumqluPGH2/atGl2ra2tLVnPpWu4tI7ccyy3d3ft2pWs51LJtm/f3q26JG3ZssWuDRw4MFlvaGiwPS7pJHc/o0ePTtZzKTGNjY12zZ0jtwcl/z3l9nQuqcYdey75yc1GuedJbs0dQ+557Hpy59vhHXYAAACgYAzsAAAAQMEY2AEAAICCMbADAAAABWNgBwAAAArGwA4AAAAUjFjHPpCLQnLRZ7kIIBfFuHr1atvT0tKSrLv4LUlatGiRXXvJS16SrL/xjW+0PbkoK6Cv5GK9/uRP/iRZd9FrUm3RjbVEE+bux91eLmqxlmPI3Z5by8X0uRg8yccI5m4PB8/LXvYyu3bSSScl64MHD7Y97rUoF6nY3t5u11wMYmtrq+1xkYG5GL9aYidzz3P3Opm75ri13P7MHYNby80ytchde933lDsGFw1aS8RsTu4YNm/enKznorodrmwAAABAwRjYAQAAgIIxsAMAAAAFY2AHAAAACsbADgAAABSMlJhDRO5TyG5t9+7dtsd9Gnvbtm3dO7D9HMO6detsT24N6Cu5lIfhw4cn67lElVpSYnpL7rhrSYnpabljcOlY6Bs//OEP7dqUKVOS9ZkzZ9qeUaNGJetjx461PXV1dXbNJYPs2bPH9nR0dCTr48aNsz251+rcfTnuepS7Tu3atStZz80EuXPnvqdcqou77uWOIbfmkmpyx+C487O/NZf4kksnctcptydyeIcdAAAAKBgDOwAAAFAwBnYAAACgYAzsAAAAQMEY2AEAAICCMbADAAAABSPWsQ+4eKnc2saNG23P4sWLk/XVq1fbnkmTJiXrO3futD0NDQ12bfDgwcn62rVrbU9uDegruSjB0aNHd7snF+Xm+no6UjEXAVeLWqIqa/mecudu+/btyXrJMZqHs8cff9yuXX/99cn6hAkTbM+wYcOS9ZEjR9qeESNG2LXGxsZu3Y/kX9dysY4ufjCnp+NV3b7Jvb7n7mfTpk3Juou9lHxMZG5/5mKl3feUOwbXs2XLFtvjohslqbW1NVnPRUG6a+8jjzxie+xtdbsDAAAbVNqdAAAgAElEQVQAQK9hYAcAAAAKxsAOAAAAFIyBHQAAACgYAzsAAABQMAZ2AAAAoGDEOvaBXKyj42KVJOnZZ59N1ltaWmzPtGnTkvXhw4fbnlw0nItqyh23i2UDSjVo0KBkvadjGHs6mrCnj68nb6+W66GUj1JD78s9jg8//HCP3Y+LC5SkpqYmu+YiGnM9LgoyF3Gce510+2bAAD+K1RL96o4hF4G4e/duu+ZipXN7sL6+PlnPHXcudtIdX+643dqOHTtsTy5asr29PVnvrShZ3mEHAAAACsbADgAAABSMgR0AAAAoGAM7AAAAUDAGdgAAAKBgpMT0gdyn6d2nu3OfPO/fv3+y7j7RLEmrVq1K1keNGmV7cp8IHzNmjF0DDiV79uyxa/Pnz0/Wp06dantcWkKO29OSvxbkjrsneyR/fLm0BJfYkPtec4kSQ4YM6XaPW+utlAf88XKvQ7lktNwacCjgHXYAAACgYAzsAAAAQMEY2AEAAICCMbADAAAABWNgBwAAAArGwA4AAAAUjFjHwuSi1JxNmzYl608++WS3b8tFr0lSXV2dXTvmmGOS9Re96EXdPgagL+XiUL/4xS8m6+PGjbM9s2fPtmuNjY3JuosslPw+zMXdObnrzcCBA+2ai0fs6OiwPe74cse9Y8cOu9ba2pqs5yIa3febi9ol8hFACXiHHQAAACgYAzsAAABQMAZ2AAAAoGAM7AAAAEDBGNgBAACAgoXe/AR8v379+Lj9QdDQ0JCsjxo1yvbs3LkzWXeJM5I0adIku+YSJZqbm21PW1ubXUNeZ2dnOqajICGEF8x+r6+vt2svf/nL7dq8efOSdZe6JEnjx49P1ocNG2Z7XOrMgAE+KCy35tKk1qxZY3ueeuqpZH3hwoW259e//rVdmz9/vl1zXLpN6UkwMUb2O/AC4fY777ADAAAABWNgBwAAAArGwA4AAAAUjIEdAAAAKBgDOwAAAFAwBnYAAACgYL0a6wgAAACge3iHHQAAACgYAzsAAABQMAZ2AAAAoGAM7AAAAEDBGNgBAACAgjGwAwAAAAVjYAcAAAAKxsAOAAAAFIyBHQAAACgYAzsAAABQMAZ2AAAAoGAM7AAAAEDBGNgBAACAgjGwAwAAAAVjYAcAAAAKxsAOAAAAFIyBHQAAACgYAzsAAABQMAZ2AAAAoGAM7AAAAEDBGNgBAACAgjGwAwAAAAVjYAcAAAAKxsAOAAAAFIyBHQAAACgYAzsAAABQMAZ2AAAAoGAM7AAAAEDBGNgBAACAgjGwAwAAAAVjYAcAAAAKxsAOAAAAFIyBHQAAACgYAzsAAABQMAZ2AAAAoGAM7AAAAEDBGNgBAACAgjGwAwAAAAVjYAcAAAAKxsAOAAAAFIyBHQAAACgYAzsAAABQMAZ2AAAAoGAM7AAAAEDBGNgBAACAgjGwAwAAAAVjYAcAAAAKxsAOAAAAFIyBHQAAACgYAzsAAABQMAZ2AAAAoGAM7AAAAEDBGNgBAACAgjGwAwAAAAVjYAcAAAAKxsAOAAAAFIyBHQAAACgYAzsAAABQMAb2PhRCuCqEcFNfH0cJQgizQgjzQwjhAL52Tgjhl71xXEBPYb//Dvsdhzv2+++w33sGA3svCCG8qfpk3RpCWBNCuCOEcFofHMeyEMLZvX2/B+iTkv41xhhDCPUhhOtCCMtDCFtCCAtCCOfu/cIY4yJJrSGE1/Xd4QJp7PcD8r/7XZJCCJdVz1l7COH/7/qF7HeUjP1+QPbd7zdVz1VbCOGpEMI7934h+91jYD/IQghXSPo/kj4jaZykKZK+LOn8vjyuUoQQBoQQJkg6Q9Kt1fIASSskvULSMEkflfTNEMK0Lq03S3p37x0psH/s9zyz3yVptaRPSbretLLfURz2e15mv18taVqMcaik10v6VAjhJV3W2e8JDOwHUQhhmKR/kvT/xRi/G2PcFmPsiDHeHmP8UOLrvxVCWBtC2BxCuD+EMLvL2nkhhMer7zivCiFcWa2PDiH8IITQGkLYGEL4eQjhDx7XEMKNqlxMbq++E/C31fopIYRfVvsXhhBe2aXn3hDCJ0MIv6je710hhNHVtYbqT8kt1d6HQgjjqmsTQwi3VY/nmRDCX3e5zatCCN+u9rZJequkP5X02xjjTkmqnqerYozLYoydMcYfSHpOUtcNfa+ks0II9bU8NkBPY7/Xtt8lqXq+bpXUYk7vvWK/oyDs9z9qvy+OMbbv/Wv1z1FdvqV7xX7/AwzsB9c8SQ2SvneAX3+HpGMkjZX0W1V+ytzrOknvjjE2STpe0j3V+gclrZQ0RpWf8D+sypNfIYQvhxC+LEkxxr+U9Lyk18UYG2OM14QQJkn6oSrvbI2UdKWk74QQxnS53zdJelv1mOqqXyNJb1Hl3e/JkkZJeo+kHdW1W6rHNFHSn0v6TAjhzC63eb6kb0saXv0eT5D0pDsp1QvFDEmL99ZijKskdUg61vUBvYz93gP7PYX9jgKx3/+I/V49/u2SnpC0RtKP9q6x39MY2A+uUZKaY4y7D+SLY4zXxxi3VH/yvErSidWf4qXKk3dWCGFojHFTjPG3XeoTJE2t/nT/873/TizG+N4Y43szd/lmST+KMf6o+k72TyTNl3Rel6/5vzHGp2KMOyR9U9KLutzvKElHxxj3xBgfjjG2hRAmSzpV0t/FGHfGGBdI+pqkv+pymw/GGG+t3ucOVTb2ltQBhhAGqrLpb4gxPrHP8pZqL1AC9vsfud/3g/2OkrDf/4j9Xj32JkmnS/qupPZ9voT9vg8G9oOrRdLoEMKA/X1hCKF/COGzIYRnq79KWlZdGl393z9TZaMtDyHcF0KYV63/i6RnJN0VQlgaQvj7bhzfVEkXVX/l1RpCaJV0mioXiL3Wdvn/2yU1Vv//jZLulHRLCGF1COGa6nA9UdLGGGPXDbpc0qQuf1+xz3FsUmXj/p7qr/5ulLRL0mWJ42+S1Lqf7xHoLez3ipr2+wFgv6Mk7PeKmvd79YeBByQdIelv9llmv++Dgf3gelCVnxovOICvfZMqv0o6W5VfRU2r1oMkxRgfijGer8qvrm5V5adhVX9i/2CMcboqH964IoRwlrmPuM/fV0i6McY4vMufITHGz+7vYKs/7X8ixjhL0p9Ieq0qP2WvljQyhNB1g06RtCpzHItU+Scv/yuEEFT5NeE4SX8WY+zYZ32SKr/C69av1oGDiP1e0e39vj/sdxSI/V7RE/t9gLr8G3b2exoD+0EUY9ws6R8lfSmEcEEIYXAIYWAI4dwQwjX7fHmTKpu/RdJgVT51LkkKIdSFEC4NIQyrDq5tkjqra68NIRxdHXA3S9qzdy1hnaTpXf5+k6TXhRDOqb4D0BBCeGUI4Yj9fW8hhDNCCCeEEPpXj6dDUmeMcYWkX0q6unp7cyS9o3pfzk8kvTiE0NCl9p+SZqryb/J2JHpeIeme+LsPrgB9iv1e+34PlTSJBkn9Je09tq7vXLLfURT2e237PYQwNoTwFyGExupxnSPpEkk/7dLDfk9gYD/IYoyfk3SFKtGEG1T5qfcy/X7EkSR9XZVfLa2S9LikX+2z/peSllV/nfYeSZdW68dIulvSVlV+4v9yjPFnkhRCuDaEcG2X27ha0kervx67srr5zlflgyx7j+1DOrDnxXhVPljSJmmJpPtU+TWaVNl801T5afx7kj4eY7zb3VCMcZ0qH7I5v3rcU1WJdHqRpLWh8qn3rSGES7u0XSrp2j+4MaAPsd+7v9+rPqrKh9r+XpV/e7ujWtuL/Y7isN9r2u9RlX/+slKVfy7zr5IujzHe1qWN/Z4Qqp9fAPpUCGGWpBsknRz386Ss/lT/XzHGebmvA1Am9jvwwsF+7xkM7AAAAEDB+CcxAAAAQMEY2AEAAICCMbADAAAABWNgBwAAAAq23/9CV0/q168fn3AFekBnZ2fo62PYnxBCr+z3GTP8f5Nj7Nixdu2BBx7osWN4+OGH7drb3vY2u3bZZan/gK90+eWX257t27cf+IHhsBBjLH6/H46v7/36pd/TnDlzpu3ZsGGDXWtubk7WOztdtLo/hpzc7aF87vWdd9gBAACAgjGwAwAAAAVjYAcAAAAKxsAOAAAAFIyBHQAAAChYiLH3Pth9OH6KHOgLL8SUmHPPPTdZv/POO23Pl770Jbv2/PPPJ+tTpkyxPd/85jeT9SuuuML2TJ482a5dffXVyfrcuXNtz5VXXmnXumvAAB8UNnr0aLs2Z86cZP20006zPfPmzUvWp02bZnu2bt1q11pbW5P1oUOH2p5ly5Yl63V1dbbnpptusmvf+973kvVdu3bZnlqQEnPw5FJY3B5497vfbXvuuusuu/bII48k67nnizu+WpJgakmcqfW+UDtSYgAAAIBDEAM7AAAAUDAGdgAAAKBgDOwAAABAwRjYAQAAgIL5iAAAOATMmDHDrl166aV2zaWM5NJRXHrL4MGDbc/u3bvt2vvf//5k/eGHH7Y9b3nLW+xad+VSI9auXWvXRo4cmaxv2rTJ9syfPz9Zf+aZZ2xP7ty5ZI1c4sv69euT9eHDh9uez3zmM3Zt4cKFyfoTTzxhe1CW3B444ogjkvXx48fbnv79+/foMdTC3V7ufnJ7DWXgHXYAAACgYAzsAAAAQMEY2AEAAICCMbADAAAABWNgBwAAAArGwA4AAAAUjFhHAIeEgQMHJuu5GL8BA/wlLoSQrO/cudP2uFg0FxEpSdu3b7drRx99dLJ+yimn2B4Xv5aLZXNrmzdvtj0tLS12rZYYOne+czF4ucdi27ZtPXZ7EydOtD3HHnusXTv++OPtGg4NuefylClTknUXKSr556UkdXZ2Juu565STO4bu3j8ODbzDDgAAABSMgR0AAAAoGAM7AAAAUDAGdgAAAKBgDOwAAABAwUiJAXBIGDVqVLKeS2jJJUC4NJFckoJLgKivr7c9zz33nF3bsWNHsp5LvmlqaurWsUk+hWLdunW2J3ceGhsbk/U9e/bYnhhjsp5Lt8klYWzZssWuOe6+cikdtSR44PDgrjlr1661PW5PS32f0tLX948/Du+wAwAAAAVjYAcAAAAKxsAOAAAAFIyBHQAAACgYAzsAAABQMAZ2AAAAoGDkVQE4JIwePTpZz8X75WIdXXRiLsbPRUEOHTrU9mzfvt2uLVu2LFmfOnWq7XGxjh0dHbZn8ODByXouPnLz5s12raGhIVnPxca5SMXcY5S7PdeXOw/uGNz5kfKPbe784dDnYkpzz4k3vvGNds1dC55//nnb8/DDDyfrS5cutT25PVUL4iDLwDvsAAAAQMEY2AEAAICCMbADAAAABWNgBwAAAArGwA4AAAAUjIEdAAAAKBixjgAOCePGjUvWc7GJuXgzF9+4c+dO27N+/fpkvbGx0fbMnTvXrjkjRoywa/X19cn6kCFDun0/EydOtGsuAlHykY/u2CT/WOTO965du7q9lotabGtrS9Zz32tOT8fnoSwuIvScc86xPW95y1vs2pgxY5L1u+++2/Z85CMfSdY/97nP2R6HeMZDG1cbAAAAoGAM7AAAAEDBGNgBAACAgjGwAwAAAAVjYAcAAAAKVkxKjPtkf+4T/26tpz+5n7s9t5b7NHYtx9fTn+52t5dLZSghEaGWNIdcj1vLnW8+ad83hg4d2q26lH+sXLrMs88+2+1jaG1ttT0uUUWSJk2alKznEl9cqsq2bdtsj7u9wYMH257jjjvOrrlztGfPHtvjrh+1JMFI0pYtW5L1Y4891va41A+XGPTHrOHQkHtdc6kuDz30kO15+9vfbtfcc/bmm2+2PdOnT0/Wx48fb3tWrlyZrOe+V17Xytf3ExgAAAAAi4EdAAAAKBgDOwAAAFAwBnYAAACgYAzsAAAAQMEY2AEAAICCFZNJdd555yXrl19+ue358z//82R96tSptsdFouUMHz7crh111FHJei7mbeTIkXbNxYTlbs/F0+Uix5YtW5asX3/99bZnxowZdq2WSCgXkbd+/Xrbc++999o19/0uWLDA9tx///3J+qJFi2xP7rFwejoa9IWooaEhWc89z3ORni4ycMOGDbanvb09WR84cKDtyR2fi1XMxS325PMit9dykbr9+/dP1nN7zV17Z8+ebXtGjx5t17Zu3ZqsNzY22h4XO1nLnpb8cxKHh3HjxiXr7vVTkm699Va7tnz58mQ9F0V69tlnJ+tvfvObbc9nP/vZZL3WayWvRWXgHXYAAACgYAzsAAAAQMEY2AEAAICCMbADAAAABWNgBwAAAArWqykxH//4x+3a3/zN3yTrP/jBD2zPxRdfnKy79AfJJwtI0saNG7t9e2vXrk3Wc5+4ziXVuMSQnv4Ed1NTU7J+6aWX2p7JkyfbNZd8k0vYcd9rLp1iypQpds09Tscdd5ztefWrX23XnMcff9yuffe7303W7777btuTe2zxO4MGDUrWcwk8uXPrUopyCS0dHR3Juksf2R93LWhra+v2bdXX19s1dx5yiSpLly61ay5tIrfff/zjHyfruUSm3GuGO3fuMZKk8ePHJ+u5NJqJEyfatVGjRtk1HL7ctUPyCUqSf862tLR0u2fOnDm2x6l1jiDJrAy8ww4AAAAUjIEdAAAAKBgDOwAAAFAwBnYAAACgYAzsAAAAQMEY2AEAAICC9Wqs46233mrXHn300WT9ySeftD0xxmR9xIgRticXfebiBHMxYQ0NDcn6EUccYXvGjh1r16ZNm5as52LH3DHk7set5WLZcnF3tUTkuWi4XOzl+vXr7drq1auT9ebmZtvjojzdOZXyj+073/nOZD0XH+miS3NRkC9E7jHJPcdCCN2+n+3bt9s1F2OWi0ecOnWqXXP7LbcH3Pc0cOBA2+MiT3Pnx+0nyX+/Li5Wkk4++eRkPRfjmovsdJG6uRjXT3/608n6e97zHttz4okn2rU777zTruHQ197enqzn5ohauNchyb9+uYjSnNx+qiWiMXd7PXk/qOAddgAAAKBgDOwAAABAwRjYAQAAgIIxsAMAAAAFY2AHAAAACsbADgAAABSsV2MdFy5caNf+7d/+LVk/7bTTbM/IkSOT9dbWVtuTi1/LRfk5LkLyRz/6ke0ZMmSIXTv99NOT9Vx03VNPPZWs5yKXXJxcLtYx5y/+4i+S9VwMo4uuyz0O55xzjl3bunWrXXMWL17c7dvavXu3XXNRlbmeF7/4xcm6i+J7oXLno9aYMBeltm3btm7flouYlfLXHBermIuSzcU3Om5PLVu2zPa0tLTYNXc9ysVRuuvelClTbE9u36xatSpZz10/XDzutddea3uuu+46u4bDm9uHuee5i4LMyd2emzHOP/982+NilnOva+61S/LX2NyM4dZy1+ta114oeIcdAAAAKBgDOwAAAFAwBnYAAACgYAzsAAAAQMEY2AEAAICC9WpKzKtf/Wq7duuttybrO3bssD3f+ta3kvW6ujrbk0sQcJ9qzvVMnz49Wb/hhhtsTy6RwCUmnHHGGbbnN7/5TbK+ZMkS27N06dJkPZdMkkts2LJlS7Le1NRke1x6xrp162xPLgHIPU5Tp061Pe57co+rlD9HuU/7d/f2cs+TFyL3fMmlsOQSG55//vlkvbm52fa4dJTc8zyXzOCes7n0hT179iTruaQalzSROz+5veYSG3J7wyVQuUQLqba0plzyzWte85pk/d5777U9eOFyiUy5JKnLLrvMrm3fvj1Zv/POO21P//79k/WJEyfantmzZ3f7fnoaqS49j3fYAQAAgIIxsAMAAAAFY2AHAAAACsbADgAAABSMgR0AAAAoGAM7AAAAULBejXV873vfa9dWrFiRrOdiwmqJDcr1DBjQ/dOxfv36ZP3YY4+1PS5SUZLuvvvubt+ei3fasGGD7Vm5cmWyvmbNGtuTi9h0UXgXXHCB7XHRU/fdd5/tefbZZ+2ai1TMnYdaYhhzzyEXNZeLxHTPoVtuuaV7B3aYGzt2bLLuzp+UjyZ0a7nHysU6umOT8rGwtXDPv9xxuyhIFxkn+e9V8uc8971Onjw5Wc+du6eeesquuTjPRx991PacddZZyXru+pqLnWxra7NrODTkIlSHDBmSrHd0dNiev/u7v7Nrp5xySrL+ta99zfZ873vfS9bvuece27Nw4cJk/Wc/+5ntyc1a7lowevRo2+OuBblr8saNG+1aLa/VhxveYQcAAAAKxsAOAAAAFIyBHQAAACgYAzsAAABQMAZ2AAAAoGAhxthrd/bAAw/YO3vkkUeS9SOPPNLenks4qKur6+aR5dXX19u1gQMHdvsYcskH7tPdgwYNsj0zZsxI1nOffnef7v7Od75je1yyjCQ99thjyfq0adNsz0tf+tJk3X0yX/JpQpJ/DuXSf9xzaPPmzbanpaXFrq1duzZZX7Zsme257bbbkvX3ve99tqezszPYxUKEEHr04vKtb30rWb/55pttz0knnWTXXIpBLulk6tSpyfqsWbNsT27v1iKX7OJs3bo1WX/88cdtTy4lxt3e+PHjbc/RRx+drOfSH9z1UPLX3uXLl9uec889N1nPXade8YpX2LVc6kZPijEWv9/79evXe8NED8o9z6+++upu98ydO9euveMd70jWjz/+eNtz++23J+vueihJp59+erI+c+ZM25PbN27G+NM//VPb464FTz75pO1ZtGiRXfvtb39r1w437vWdd9gBAACAgjGwAwAAAAVjYAcAAAAKxsAOAAAAFIyBHQAAACgYAzsAAABQMJ93dxD85je/sWvbt29P1nMRZi62cNeuXbbHxZFJ0u7du+2as2fPnmS9o6PD9uQioSZPnpysu/Mj+e/XRTdK0qpVq5L1l7zkJbbnxBNPtGtnnHFGsv7+97/f9rh4yyuvvNL2zJs3z665+MYnnnjC9mzYsCFZz8W15eKlnnnmmWQ99z19+9vftmv4nYsuuqjbPbkI1csvvzxZv/TSS22P2x+55+WcOXPsmout7ezstD3OcccdZ9daW1uT9VxE6Zo1a+zamDFjkvWJEyfaHicXJ+eur5I0bNiwZD13rXRxrR//+Mdtj/tepd6LdcTBk9tr7rkUgk/Z/Ku/+qtuH8MXvvAFu+Zed//2b//W9rio1He+852259prr7Vrs2fPTtZzkbXt7e3Jeu5aeeqpp9q1v/7rv7ZrLxS8ww4AAAAUjIEdAAAAKBgDOwAAAFAwBnYAAACgYAzsAAAAQMFCjLHX7uzss8+2dzZ9+vRk3aWPSD4lxqWFSPmEFrc2fPhw2+PSF3JJNW1tbXbNHfvIkSNtj0vf+cAHPmB7hg4dmqx/7GMfsz1Tpkyxay4d5Yc//KHtcedu/fr1tueyyy6za9/85jeT9dz5dslAucSg3JpLHMj1uOeKe35Xb8/HFBQihNB7Fxcjdw4bGhqS9bq6Otvj9mfumpM7hlxfd+XSF9we+NznPmd7Vq5cadfc83zGjBm2Z9u2bcn6okWLbI+7Tkk+JSaXbjNu3Lhk/aijjrI9uWt5rq8nxRiL3+/9+vXr8/1eC3cdkHySVFNTk+35+te/bteeffbZZP2ss86yPe41+d3vfrfteeUrX5msf+c737E9d911V7dvz6VcSX4ffuMb37A9b3/72+3a0qVL7drhprOzM7nfeYcdAAAAKBgDOwAAAFAwBnYAAACgYAzsAAAAQMEY2AEAAICCMbADAAAABeu5TLED0NLSYtdcZODo0aNtj1vLRTfmIttcVNmyZctsj4smzEVB5uIR3fHljnvOnDnJ+pVXXml73vrWt3b72HIRdAsXLkzW29vbbY+Lo/zoRz9qe6699lq75h6LXKSi09PPoVqOwd3WC1UuHtGpJTYx91i5eMQSHqvm5ma7dvLJJyfrv/zlL23P008/bddOO+20ZP2Tn/yk7bnjjjuSdXf9kqQPf/jDds09ti7uUfIRjfX19banlteg3GOBQ0dHR0eyPnbsWNszZMgQu+auLT//+c9tz6WXXpqsz5w50/aceeaZyfq9995rez70oQ/ZNRc7+a53vcv2uNfxd7zjHbbn+eeft2vgHXYAAACgaAzsAAAAQMEY2AEAAICCMbADAAAABWNgBwAAAArGwA4AAAAUrFdjHZcvX27XRowYkazfeOONtue2225L1p944gnb4+LIJB83dMMNN9geF5/0ile8wvY8+OCDds3FxuWiolwE4QknnGB7GhoakvVnn33W9jzwwAN27c4770zWr7vuOttz8cUXJ+sLFiywPRs3brRrLuYtF+3n4vhc/FuuJ7dWS6wjfl8t0Ym5817L7dXSU0scZS22b99u1+6///5kPXetvOSSS+za9ddfn6zPmjXL9rjHYv78+bZn586ddq2xsTFZz8WuutvLPa7uWinl419x6HPPl5EjR9qegQMH2jV3Lcg9z1esWJGsr1mzxva418lp06bZntNPP92uudfkxx57zPb8+te/tmuoDe+wAwAAAAVjYAcAAAAKxsAOAAAAFIyBHQAAACgYAzsAAABQsF5NiWltbbVrX/nKV5L1sWPH2p4LLrggWb/33nttzy233GLX3CehX/7yl9ue448/PlmfN2+e7fnABz5g14YOHZqsNzc32x73yfNXvvKVtmfOnDnJei7V5Z//+Z/t2vTp05P1a665xvY89dRTyXouVaOWhJae7iHx5dBRy2Pfm8dQS4JMT6bb5FKXvvCFL9i1//iP/0jWc8kQLqHrf/7nf7p9P5J01llnJeu5VKhahBDsmrvu4dBRy7V++PDhtmfQoEE13ZfjrhG5Y7jwwguT9auvvtr2jB8/3q65pKRaUpJqTc3qret1yXiHHQAAACgYAzsAAABQMAZ2AAAAoGAM7AAAAEDBGNgBAACAgjGwAwAAAAXr1VjHnA9/+MPJ+nnnnWd7TjnllG7VJWn+/Pl27T3veU+yXldXZ3u+9rWvJetf/vKXbc/atWvt2tSpU7vds2vXrmT9/vvvtz25uEXnn/7pn6/y5ZsAAAjTSURBVOza0qVLk3V3bFLt8U4OcYs41ByqUWX/+I//mKzv3LnT9rztbW9L1ocMGWJ7Tj/9dLvmonNruebkrkUxRrvmrtc4PLS3tyfruejG/v372zU3SxxxxBG25+STT07WL7nkEtvzvve9L1lftGiR7ck9l91+z+1PF8ldS0S1dOheK3sS77ADAAAABWNgBwAAAArGwA4AAAAUjIEdAAAAKBgDOwAAAFCwXk2JyX0C+Nvf/naynkv+OPPMM5P10aNH257Vq1fbNWfAAH+a3Pf0+OOP257Zs2fbtTe84Q3J+ne/+13b89Of/jRZzyXivPa1r03Wn3jiCduT4z7BnXvM3RppL0DvyCWq5K57GzduTNbvuOMO2/PVr341WZ8zZ47tuemmm+zaXXfdlaxPmjTJ9jQ0NCTruetUW1tbTWs4NOTSRzZs2JCsz5gxw/a413BJeulLX9qtuiSde+65yXouJeZVr3pVsj5x4kTb8/nPf96ubdu2LVl/61vfanvcfHbLLbfYHuTxDjsAAABQMAZ2AAAAoGAM7AAAAEDBGNgBAACAgjGwAwAAAAVjYAcAAAAKFmKMvXZndXV19s5clF8ubuviiy9O1o8//njbc//999s1FxOWizdzkVC5qKicK664Ilm/7LLLbM/JJ5+crDc3N9sed15zx93T5yH32Hb3fl5oOjs7Q18fw/6EEHrv4oIeN3jwYLvm4iAvuugi2/Pggw8m66eeeqrtOf300+3a4sWLk/ULL7zQ9rjvKRcFuXPnTrt2zDHH2LWeFGMsfr/369fvkNzvude1WbNmJeuNjY22Z9CgQXZt5MiRyfrQoUNtz7Jly5L1LVu22B73PeVmguHDh9u1pqamZP2MM86wPXPnzk3WXUylxOv7Xu71nXfYAQAAgIIxsAMAAAAFY2AHAAAACsbADgAAABSMgR0AAAAoGAM7AAAAULBejXU8VGOfchoaGpJ1F3smSSeddJJdGzVqVLL+7LPP2h63lotNdPFmuQgzF72J3kesIw62WmJcc9ecJUuWJOu33nqr7Xn9619v1972trcl65/61Kdsj4uua2trsz39+/e3a7nYyZ5ErOPB417DJenyyy9P1n/2s5/ZngkTJti1DRs2JOs7duywPc7AgQO73TNjxgy7ltsDN998c7L+zne+0/Z89KMfTdbPPPNM27N+/Xq79kJCrCMAAABwCGJgBwAAAArGwA4AAAAUjIEdAAAAKBgDOwAAAFAwHwOAA5JLVXEWLlzY7R6XylBrz9atW7t9ewBeOHLXD7eW61m6dGmyvnHjRttz1lln2bXt27cn67k0q8bGxmT9+9//vu0ZN26cXcOhL5eGNHfu3GT9nnvusT319fV2bfLkycn6sGHDbM+IESOS9VzCn9trY8eOtT1f/epX7dqUKVOS9VxK0nHHHZes51Ly7rzzTrsG3mEHAAAAisbADgAAABSMgR0AAAAoGAM7AAAAUDAGdgAAAKBgDOwAAABAwYh17AO52DEAKEEtUbI569evT9ZXrlxpe3KRe05dXZ1dczG8V199te3JRUvi8Oaef7fccovt+dKXvmTXFixYkKy3tLTYnsGDByfrxx9/vO25+eabk/UPfOADtufEE0+0a0cffXSynou17tcv/X7wtGnTbA/yeIcdAAAAKBgDOwAAAFAwBnYAAACgYAzsAAAAQMEY2AEAAICCkRIDAPgDLuVBqi1Bpn///t2+rYaGBrs2b968ZD2XErNs2bJk/brrrrM973//++0aDm+7du1K1nN7Y/HixXbtpz/9abePwd3Xz3/+c9vz8MMPJ+sXXXSR7fngBz9o19785jcn60uWLLE9N9xwQ7L+k5/8xPYgj3fYAQAAgIIxsAMAAAAFY2AHAAAACsbADgAAABSMgR0AAAAoGAM7AAAAUDBiHQEAfyAXXefkIhqbmpqS9SlTptiebdu22bXTTjvtwA+sysXdbd261fa0trbaNXeOaom9RN/IPVYuBvScc86xPbko0lq451juOXvvvfcm64sWLbI9c+fOtWsXXnhhst7c3Gx7vvjFLybr7pxKPR8le7jhHXYAAACgYAzsAAAAQMEY2AEAAICCMbADAAAABWNgBwAAAArGwA4AAAAUjFhHAMBBt3v37mS9sbHR9uSi5iZMmJCs79q1y/ZcddVVyfoZZ5xhe3B4y8UFPv/888n6gAF+dBo4cGC3j6GWCNUc9z2tXbvW9nz/+9+3a7fffnu37kfy3xPRjbXjHXYAAACgYAzsAAAAQMEY2AEAAICCMbADAAAABWNgBwAAAApGSgwA4A/0dGLDkiVLkvWHHnrI9lx44YV2zaXLLF682PbceOONyfqvfvUr20NyxeEt9/i2trYm6w0NDbanf//+f/QxdeWOz6Uu5eTSbXLnwa3Vkm5Ty/2ggnfYAQAAgIIxsAMAAAAFY2AHAAAACsbADgAAABSMgR0AAAAoGAM7AAAAUDBiHQEAfyAXsVZXV9ftnrvuuitZ/5d/+Rfb8+///u927eKLL07W/+Ef/sH2TJ8+PVnfuXOn7enp6DqUJfdYrVixIllvbm62PXv27Pmjj6mrnnz+5aIge+t5Xsv9oIIzBwAAABSMgR0AAAAoGAM7AAAAUDAGdgAAAKBgDOwAAABAwUKMsdfurF+/fr13Z8BhrLOzM/T1MexPCIH9fpjqyUSJ4cOH255LLrnEri1evDhZf+KJJ2zP+vXrk/Vakyt6Kw0mxlj8fj8cX9/d8yL3vHzwwQft2tKlS3vsGEgiOny513feYQcAAAAKxsAOAAAAFIyBHQAAAPh/7dwxDQAADMMw/qyHoc+Uw0YR9WiYYAcAgDDBDgAAYYIdAADCXm8dAQCAjYUdAADCBDsAAIQJdgAACBPsAAAQJtgBACBMsAMAQJhgBwCAMMEOAABhgh0AAMIEOwAAhAl2AAAIE+wAABAm2AEAIEywAwBAmGAHAIAwwQ4AAGGCHQAAwgQ7AACECXYAAAgT7AAAECbYAQAgTLADAECYYAcAgLADrIi1RSx5VqUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 921.6x691.2 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    images_rotated = []\n",
    "    labes_rotated = []\n",
    "\n",
    "    for img in images:\n",
    "        rotated_imgs = [\n",
    "            img,\n",
    "            rotate(img, 90),\n",
    "            rotate(img, 180),\n",
    "            rotate(img, 270)\n",
    "        ]\n",
    "        rotation_labels = torch.LongTensor([0, 1, 2, 3])\n",
    "        stack = torch.stack(rotated_imgs, dim=0)\n",
    "\n",
    "        images_rotated.append(stack)\n",
    "        labes_rotated.append(rotation_labels)\n",
    "\n",
    "    images = torch.cat(images_rotated, dim=0)\n",
    "    labels = torch.cat(labes_rotated, dim=0)\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    \n",
    "    f, axes = plt.subplots(2, 3, figsize=(12.8, 9.6))\n",
    "    axes = [ax for axs in axes for ax in axs]\n",
    "\n",
    "    for i in range(6):\n",
    "        index = randint(0, len(images))\n",
    "        image = images[index]\n",
    "        label = labels[index]\n",
    "        axes[i].imshow(image[0].view(32, 32), cmap='gray')\n",
    "        axes[i].set_title('Class:' + str(label))\n",
    "        axes[i].axis('off')  \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplar CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_flip(image):\n",
    "    \"\"\"Flip image horiontally.\"\"\"\n",
    "    transform = Compose([\n",
    "        RandomHorizontalFlip(p=1.0),\n",
    "        Resize(32),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=(0.5,), std=(0.5,))\n",
    "    ])\n",
    "    img = transform(image)\n",
    "    return img\n",
    "\n",
    "\n",
    "def random_crop(image):\n",
    "    \"\"\"Crop Image.\"\"\"\n",
    "    transform = Compose([\n",
    "        RandomCrop((20, 20)),\n",
    "        Resize(32),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=(0.5,), std=(0.5,))\n",
    "    ])\n",
    "    img = transform(image)\n",
    "    return img\n",
    "\n",
    "\n",
    "def color_jitter(image):\n",
    "    \"\"\"Apply color jitter.\"\"\"\n",
    "    transform = Compose([\n",
    "        ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.02),\n",
    "        Resize(32),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=(0.5,), std=(0.5,))\n",
    "    ])\n",
    "    img = transform(image)\n",
    "    return img\n",
    "\n",
    "\n",
    "def random_resized_crop(image):\n",
    "    \"\"\"Randomly resize and crop image.\"\"\"\n",
    "    transform = Compose([\n",
    "        RandomResizedCrop(40, scale=(0.2, 1.0), ratio=(0.75, 1.333)),\n",
    "        Resize(32),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=(0.5,), std=(0.5,))\n",
    "    ])\n",
    "    img = transform(image)\n",
    "    return img\n",
    "\n",
    "\n",
    "def random_rotation(image):\n",
    "    \"\"\"Randomly rotate image.\"\"\"\n",
    "    transform = Compose([\n",
    "        RandomRotation(45),\n",
    "        Resize(32),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=(0.5,), std=(0.5,))\n",
    "    ])\n",
    "    img = transform(image)\n",
    "    return img\n",
    "\n",
    "\n",
    "def random_affine_transformation(image):\n",
    "    \"\"\"Applies a random affine transformation to the image.\"\"\"\n",
    "    transform = Compose([\n",
    "        RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.3), shear=10),\n",
    "        Resize(32),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=(0.5,), std=(0.5,))\n",
    "    ])\n",
    "    img = transform(image)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    \"\"\"Flattens a tensor.\"\"\"\n",
    "    return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class CifarNet(nn.Module):\n",
    "    \"\"\"CifarNet model\"\"\"\n",
    "\n",
    "    def __init__(self, input_channels, num_classes=10):\n",
    "        super(CifarNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=(5, 5), bias=False)\n",
    "        self.max1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        self.batch1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=(5, 5), bias=False)\n",
    "        self.batch2 = nn.BatchNorm2d(num_features=64)\n",
    "        self.max2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        self.flat = flatten\n",
    "        self.fc1 = nn.Linear(1600, 384, bias=True)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(384, 192, bias=True)\n",
    "        self.fc3 = nn.Linear(192, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.max1(out)\n",
    "        out = self.batch1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch2(out)\n",
    "        out = self.max2(out)\n",
    "        out = self.flat(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(image, transformation):\n",
    "    \"\"\"Randomly transforms one image.\"\"\"\n",
    "    transform = ToPILImage()\n",
    "    img = transform(image.cpu())\n",
    "\n",
    "    if transformation == 0:\n",
    "        return horizontal_flip(img)\n",
    "    if transformation == 1:\n",
    "        return random_crop(img)\n",
    "    if transformation == 2:\n",
    "        return color_jitter(img)\n",
    "    if transformation == 3:\n",
    "        return random_resized_crop(img)\n",
    "    if transformation == 4:\n",
    "        return random_rotation(img)\n",
    "    if transformation == 5:\n",
    "        return random_affine_transformation(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, scheduler, num_epochs, train_loader):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "\n",
    "    train_losses, train_accuracies = [], []\n",
    "    best_acc = 0.0\n",
    "    since = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        image_index = 0\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "\n",
    "        running_loss = []\n",
    "        running_corrects = 0.0\n",
    "        len_transformed_imgs = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.cpu()\n",
    "            images_transformed, labes_transformed = [], []\n",
    "\n",
    "            for index, img in enumerate(images):\n",
    "                transformed_imgs = [\n",
    "                    img,\n",
    "                    transform_image(img, 0),\n",
    "                    transform_image(img, 1),\n",
    "                    transform_image(img, 2),\n",
    "                    transform_image(img, 3),\n",
    "                    transform_image(img, 4),\n",
    "                    transform_image(img, 5),\n",
    "                    transform_image(img, 3),\n",
    "                    transform_image(img, 4),\n",
    "                    transform_image(img, 5),\n",
    "                    transform_image(img, 1),\n",
    "                    transform_image(img, 3),\n",
    "                    transform_image(img, 4),\n",
    "                    transform_image(img, 5),\n",
    "                    transform_image(img, 3),\n",
    "                    transform_image(img, 4),\n",
    "                    transform_image(img, 5),\n",
    "                    transform_image(img, 3),\n",
    "                    transform_image(img, 4),\n",
    "                    transform_image(img, 5)\n",
    "                ]\n",
    "                len_transformed_imgs = len(transformed_imgs)\n",
    "                transformed_labels = torch.LongTensor(np.repeat(image_index, len(transformed_imgs)).tolist())\n",
    "                stack = torch.stack(transformed_imgs, dim=0)\n",
    "\n",
    "                images_transformed.append(stack)\n",
    "                labes_transformed.append(transformed_labels)\n",
    "                image_index += 1\n",
    "\n",
    "            images = torch.cat(images_transformed, dim=0).cpu()\n",
    "            labels = torch.cat(labes_transformed, dim=0).cpu()\n",
    "            print(labels)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            print(preds)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # backward + optimize only if in training phase\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss.append(loss.item())\n",
    "            print(torch.sum(preds == labels.data))\n",
    "            running_corrects += torch.sum(preds == labels.data).to(torch.float32)\n",
    "\n",
    "        train_losses.append(np.mean(np.array(running_loss)))\n",
    "        train_accuracies.append((100.0 * running_corrects) / (len_transformed_imgs * len(train_loader.dataset)))\n",
    "\n",
    "        print('Epoch {}/{}: train_loss: {:.4f}, train_accuracy: {:.4f}'.format(\n",
    "            epoch + 1, num_epochs,\n",
    "            train_losses[-1],\n",
    "            train_accuracies[-1]))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "\n",
    "    print('\\nTraining complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best accuracy: {:4f}'.format(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-327-46df7d153290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loader_fashion_mnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader_fashion_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not callable"
     ]
    }
   ],
   "source": [
    "train_loader_fashion_mnist = train_loader_fashion_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   0,   0,  ..., 255, 255, 255])\n",
      "tensor([ 416, 2475, 1581,  ...,  736, 2622,  416])\n",
      "tensor(0)\n",
      "tensor([256, 256, 256,  ..., 511, 511, 511])\n",
      "tensor([1501,  688,  688,  ..., 2453, 1017, 2230])\n",
      "tensor(0)\n",
      "tensor([512, 512, 512,  ..., 767, 767, 767])\n",
      "tensor([2005, 1069, 1069,  ..., 2005,   67,  126])\n",
      "tensor(1)\n",
      "tensor([ 768,  768,  768,  ..., 1023, 1023, 1023])\n",
      "tensor([126,  19, 502,  ..., 612, 676, 220])\n",
      "tensor(0)\n",
      "tensor([1024, 1024, 1024,  ..., 1279, 1279, 1279])\n",
      "tensor([1015,   70,  394,  ...,  713,  583,  713])\n",
      "tensor(0)\n",
      "tensor([1280, 1280, 1280,  ..., 1535, 1535, 1535])\n",
      "tensor([2005,  877, 2005,  ..., 1083, 1018, 1002])\n",
      "tensor(0)\n",
      "tensor([1536, 1536, 1536,  ..., 1791, 1791, 1791])\n",
      "tensor([1111, 2005, 2005,  ..., 2005, 1098, 2005])\n",
      "tensor(0)\n",
      "tensor([1792, 1792, 1792,  ..., 2047, 2047, 2047])\n",
      "tensor([ 203, 1158, 1532,  ..., 1486, 1486, 1713])\n",
      "tensor(0)\n",
      "tensor([2048, 2048, 2048,  ..., 2303, 2303, 2303])\n",
      "tensor([1767, 1550, 1509,  ..., 1612, 1612, 1625])\n",
      "tensor(0)\n",
      "tensor([2304, 2304, 2304,  ..., 2559, 2559, 2559])\n",
      "tensor([2049, 1107, 1107,  ..., 1872, 1867, 1867])\n",
      "tensor(0)\n",
      "tensor([2560, 2560, 2560,  ..., 2815, 2815, 2815])\n",
      "tensor([2066, 2276, 2184,  ..., 2282, 2413, 2467])\n",
      "tensor(0)\n",
      "tensor([2816, 2816, 2816,  ..., 2999, 2999, 2999])\n",
      "tensor([2636, 2580, 2075,  ..., 2580,  106, 2580])\n",
      "tensor(0)\n",
      "Epoch 1/3: train_loss: 11.9065, train_accuracy: 0.0017\n",
      "tensor([3000, 3000, 3000,  ..., 3255, 3255, 3255])\n",
      "tensor([2659, 2936, 2936,  ..., 2936,  408,  324])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /Users/soumith/b101_2/2019_02_08/wheel_build_dirs/wheel_3.6/pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:93",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-328-eb080326ed0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexemplar_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_fashion_mnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-326-df922aae62ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_fn, optimizer, scheduler, num_epochs, train_loader)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# backward + optimize only if in training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 904\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1970\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1788\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /Users/soumith/b101_2/2019_02_08/wheel_build_dirs/wheel_3.6/pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:93"
     ]
    }
   ],
   "source": [
    " # number of predicted classes = number of training images\n",
    "\n",
    "exemplar_cnn = CifarNet(input_channels=1, num_classes=len(train_loader_fashion_mnist.dataset))\n",
    "exemplar_cnn = exemplar_cnn.cpu()\n",
    "\n",
    "# Criteria NLLLoss which is recommended with softmax final layer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.Adam(exemplar_cnn.parameters(), lr=0.01)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 4 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=10, gamma=0.01)\n",
    "\n",
    "train(exemplar_cnn, loss_fn, optimizer, scheduler, 3, train_loader_fashion_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-217-5711a111f140>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-217-5711a111f140>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    axes = [ax for axs in axes for ax in axs]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "f, axes = plt.subplots(2, 3, figsize=(12.8, 9.6))\n",
    "    axes = [ax for axs in axes for ax in axs]\n",
    "\n",
    "    for i in range(6):\n",
    "        index = randint(0, len(images))\n",
    "        image = images[index]\n",
    "        label = labels[index]\n",
    "        axes[i].imshow(image[0].view(32, 32), cmap='gray')\n",
    "        axes[i].set_title('Class:' + str(label))\n",
    "        axes[i].axis('off')  \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepFashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../img/'\n",
    "TARGET_SIZE = (32, 32)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "TRANSFORM_DEEP_FASHION = Compose([Resize(TARGET_SIZE), ToTensor(), Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "ROOT_DIR_DEEP_FASHION = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_loader(path):\n",
    "    return Image.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFashionDataset(Dataset):\n",
    "    \"\"\"Creates the DeepFashion data set.\"\"\"\n",
    "    def __init__(self, root, image_list, targets, transform=TRANSFORM_DEEP_FASHION, loader=default_loader):\n",
    "        self.root = root\n",
    "        self.image_list = image_list\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            image_path = self.image_list[index]\n",
    "            img = self.loader(os.path.join(self.root, image_path))\n",
    "            target = self.targets[index]\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            return img, target\n",
    "        except IndexError:\n",
    "            print('do nothing')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_list_category_img():\n",
    "    \"\"\"Load the list containing the image name and the associated category.\"\"\"\n",
    "    list_category_img = pd.read_csv('../list_category_img.txt', sep=\"\\t\", header=0)\n",
    "    list_category_img.columns = [\"image_name\", \"category_label\"]\n",
    "    return list_category_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_list_category_cloth():\n",
    "    \"\"\"Load the list containing the category name and the associated category type.\"\"\"\n",
    "    list_category_cloth = pd.read_csv('../list_category_cloth.txt', sep=\"\\t\", header=0)\n",
    "    list_category_cloth.columns = [\"category_name\", \"category_type\"]\n",
    "    return list_category_cloth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_list_eval_partition():\n",
    "    \"\"\"Load the list containing the image name and the associated evaluation status.\"\"\"\n",
    "    list_eval_partition = pd.read_csv('../list_eval_partition.txt', sep=\"\\t\", header=0)\n",
    "    list_eval_partition.columns = [\"image_name\", \"evaluation_status\"]\n",
    "    return list_eval_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data():\n",
    "    \"\"\"Return the images with the evaluation status 'train'.\"\"\"\n",
    "    partition = load_list_eval_partition()\n",
    "    labels = load_list_category_img()\n",
    "\n",
    "    result = partition[partition.evaluation_status == 'train']\n",
    "    result = result.sample(60000)\n",
    "    train_indices = result.index\n",
    "\n",
    "    list_train_images = result.image_name.values\n",
    "    train_labels = labels.category_label[train_indices].values\n",
    "\n",
    "    return DeepFashionDataset(root=ROOT_DIR_DEEP_FASHION, image_list=list_train_images, targets=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_data():\n",
    "    \"\"\"Return the images with the evaluation status 'val'.\"\"\"\n",
    "    partition = load_list_eval_partition()\n",
    "    labels = load_list_category_img()\n",
    "\n",
    "    result = partition[partition.evaluation_status == 'val']\n",
    "    result = result.sample(20000)\n",
    "    val_indices = result.index\n",
    "\n",
    "    list_val_images = result.image_name.values\n",
    "    val_labels = labels.category_label[val_indices].values\n",
    "\n",
    "    return DeepFashionDataset(root=ROOT_DIR_DEEP_FASHION, image_list=list_val_images, targets=val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data():\n",
    "    \"\"\"Return the images with the evaluation status 'test'.\"\"\"\n",
    "    partition = load_list_eval_partition()\n",
    "    labels = load_list_category_img()\n",
    "\n",
    "    result = partition[partition.evaluation_status == 'test']\n",
    "    result = result.sample(20000)\n",
    "    test_indices = result.index\n",
    "\n",
    "    list_test_images = result.image_name.values\n",
    "    test_labels = labels.category_label[test_indices].values\n",
    "\n",
    "    return DeepFashionDataset(root=ROOT_DIR_DEEP_FASHION, image_list=list_test_images, targets=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loader_deep_fashion():\n",
    "    \"\"\"Return the data loader for the train data.\"\"\"\n",
    "    return DataLoader(train_data(), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "def val_loader_deep_fashion():\n",
    "    \"\"\"Return the data loader for the validation data.\"\"\"\n",
    "    return DataLoader(val_data(), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "def test_loader_deep_fashion():\n",
    "    \"\"\"Return the data loader for the test data.\"\"\"\n",
    "    return DataLoader(test_data(), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32]) torch.Size([64])\n",
      "tensor([19,  3, 41, 32, 41, 26, 41, 16, 41, 31, 48, 33, 19, 19, 41,  3, 17, 26,\n",
      "        41, 41, 30, 41, 26,  6, 41, 41, 39, 33, 12, 41, 41, 17,  3,  3, 41, 16,\n",
      "        35, 33, 18, 41, 41, 29, 41, 41, 18, 32, 41, 33,  3, 13, 18, 41, 41, 18,\n",
      "        42,  2, 18, 32,  6,  3, 18,  6,  6, 18])\n"
     ]
    }
   ],
   "source": [
    "train_loader = train_loader_deep_fashion()\n",
    "\n",
    "for xs, ys in train_loader:\n",
    "    print(xs.size(), ys.size())\n",
    "    print(ys) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(image, angle):\n",
    "    \"\"\"Rotate the image by the specified angle\"\"\"\n",
    "    \n",
    "    image = tf.to_pil_image(image)\n",
    "    image = tf.rotate(image, angle)\n",
    "    image = tf.to_tensor(image)\n",
    "    if image.shape[0] == 3:\n",
    "        image = tf.normalize(image, (0.5,0.5,0.5,), (0.5,0.5,0.5,))\n",
    "    else:\n",
    "        image = tf.normalize(image, (0.5,), (0.5,))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "batch must contain tensors, numbers, dicts or lists; found <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-18b4552c3c94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader_deep_fashion_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: batch must contain tensors, numbers, dicts or lists; found <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader_deep_fashion_test():\n",
    "    images = images\n",
    "    labels = labels\n",
    "\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    \n",
    "    images_rotated = []\n",
    "    labes_rotated = []\n",
    "    \n",
    "    for index, img in enumerate(images):\n",
    "        rotated_imgs = [\n",
    "            img,\n",
    "            rotate(img, 90),\n",
    "            rotate(img, 180),\n",
    "            rotate(img, 270)\n",
    "        ]\n",
    "        rotation_labels = torch.LongTensor([0, 1, 2, 3])\n",
    "        stack = torch.stack(rotated_imgs, dim=0)\n",
    "\n",
    "        images_rotated.append(stack)\n",
    "        labes_rotated.append(rotation_labels)\n",
    "\n",
    "    images = torch.cat(images_rotated, dim=0)\n",
    "    labels = torch.cat(labes_rotated, dim=0)\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
